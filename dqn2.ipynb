{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 0.995\n",
    "LR = 0.001\n",
    "MEMORY_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        self.model = DQN(state_size, action_size).to(device)\n",
    "        self.target_model = DQN(state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=LR)\n",
    "        self.epsilon = EPS_START\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            action_values = self.model(state)\n",
    "        return action_values.argmax().item()\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.LongTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        dones = torch.FloatTensor(dones).to(device)\n",
    "\n",
    "        current_q_values = self.model(states).gather(1, actions.unsqueeze(1))\n",
    "        next_q_values = self.target_model(next_states).max(1)[0].detach()\n",
    "        target_q_values = rewards + (1 - dones) * GAMMA * next_q_values\n",
    "\n",
    "        loss = nn.MSELoss()(current_q_values, target_q_values.unsqueeze(1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(EPS_END, self.epsilon * EPS_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Total Reward: 48.0, Epsilon: 0.99\n",
      "Episode: 2, Total Reward: 13.0, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pravm\\AppData\\Local\\Temp\\ipykernel_2640\\1399148068.py:40: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  states = torch.FloatTensor(states).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 3, Total Reward: 37.0, Epsilon: 0.99\n",
      "Episode: 4, Total Reward: 37.0, Epsilon: 0.98\n",
      "Episode: 5, Total Reward: 16.0, Epsilon: 0.98\n",
      "Episode: 6, Total Reward: 14.0, Epsilon: 0.97\n",
      "Episode: 7, Total Reward: 15.0, Epsilon: 0.97\n",
      "Episode: 8, Total Reward: 18.0, Epsilon: 0.96\n",
      "Episode: 9, Total Reward: 27.0, Epsilon: 0.96\n",
      "Episode: 10, Total Reward: 17.0, Epsilon: 0.95\n",
      "Episode: 11, Total Reward: 27.0, Epsilon: 0.95\n",
      "Episode: 12, Total Reward: 14.0, Epsilon: 0.94\n",
      "Episode: 13, Total Reward: 12.0, Epsilon: 0.94\n",
      "Episode: 14, Total Reward: 25.0, Epsilon: 0.93\n",
      "Episode: 15, Total Reward: 20.0, Epsilon: 0.93\n",
      "Episode: 16, Total Reward: 14.0, Epsilon: 0.92\n",
      "Episode: 17, Total Reward: 16.0, Epsilon: 0.92\n",
      "Episode: 18, Total Reward: 14.0, Epsilon: 0.91\n",
      "Episode: 19, Total Reward: 14.0, Epsilon: 0.91\n",
      "Episode: 20, Total Reward: 19.0, Epsilon: 0.90\n",
      "Episode: 21, Total Reward: 14.0, Epsilon: 0.90\n",
      "Episode: 22, Total Reward: 17.0, Epsilon: 0.90\n",
      "Episode: 23, Total Reward: 34.0, Epsilon: 0.89\n",
      "Episode: 24, Total Reward: 10.0, Epsilon: 0.89\n",
      "Episode: 25, Total Reward: 20.0, Epsilon: 0.88\n",
      "Episode: 26, Total Reward: 32.0, Epsilon: 0.88\n",
      "Episode: 27, Total Reward: 17.0, Epsilon: 0.87\n",
      "Episode: 28, Total Reward: 42.0, Epsilon: 0.87\n",
      "Episode: 29, Total Reward: 22.0, Epsilon: 0.86\n",
      "Episode: 30, Total Reward: 24.0, Epsilon: 0.86\n",
      "Episode: 31, Total Reward: 28.0, Epsilon: 0.86\n",
      "Episode: 32, Total Reward: 32.0, Epsilon: 0.85\n",
      "Episode: 33, Total Reward: 12.0, Epsilon: 0.85\n",
      "Episode: 34, Total Reward: 41.0, Epsilon: 0.84\n",
      "Episode: 35, Total Reward: 27.0, Epsilon: 0.84\n",
      "Episode: 36, Total Reward: 40.0, Epsilon: 0.83\n",
      "Episode: 37, Total Reward: 29.0, Epsilon: 0.83\n",
      "Episode: 38, Total Reward: 21.0, Epsilon: 0.83\n",
      "Episode: 39, Total Reward: 17.0, Epsilon: 0.82\n",
      "Episode: 40, Total Reward: 49.0, Epsilon: 0.82\n",
      "Episode: 41, Total Reward: 56.0, Epsilon: 0.81\n",
      "Episode: 42, Total Reward: 30.0, Epsilon: 0.81\n",
      "Episode: 43, Total Reward: 23.0, Epsilon: 0.81\n",
      "Episode: 44, Total Reward: 44.0, Epsilon: 0.80\n",
      "Episode: 45, Total Reward: 34.0, Epsilon: 0.80\n",
      "Episode: 46, Total Reward: 20.0, Epsilon: 0.79\n",
      "Episode: 47, Total Reward: 46.0, Epsilon: 0.79\n",
      "Episode: 48, Total Reward: 58.0, Epsilon: 0.79\n",
      "Episode: 49, Total Reward: 13.0, Epsilon: 0.78\n",
      "Episode: 50, Total Reward: 75.0, Epsilon: 0.78\n",
      "Episode: 51, Total Reward: 50.0, Epsilon: 0.77\n",
      "Episode: 52, Total Reward: 45.0, Epsilon: 0.77\n",
      "Episode: 53, Total Reward: 28.0, Epsilon: 0.77\n",
      "Episode: 54, Total Reward: 29.0, Epsilon: 0.76\n",
      "Episode: 55, Total Reward: 44.0, Epsilon: 0.76\n",
      "Episode: 56, Total Reward: 47.0, Epsilon: 0.76\n",
      "Episode: 57, Total Reward: 33.0, Epsilon: 0.75\n",
      "Episode: 58, Total Reward: 21.0, Epsilon: 0.75\n",
      "Episode: 59, Total Reward: 44.0, Epsilon: 0.74\n",
      "Episode: 60, Total Reward: 26.0, Epsilon: 0.74\n",
      "Episode: 61, Total Reward: 100.0, Epsilon: 0.74\n",
      "Episode: 62, Total Reward: 74.0, Epsilon: 0.73\n",
      "Episode: 63, Total Reward: 23.0, Epsilon: 0.73\n",
      "Episode: 64, Total Reward: 47.0, Epsilon: 0.73\n",
      "Episode: 65, Total Reward: 36.0, Epsilon: 0.72\n",
      "Episode: 66, Total Reward: 113.0, Epsilon: 0.72\n",
      "Episode: 67, Total Reward: 49.0, Epsilon: 0.71\n",
      "Episode: 68, Total Reward: 60.0, Epsilon: 0.71\n",
      "Episode: 69, Total Reward: 51.0, Epsilon: 0.71\n",
      "Episode: 70, Total Reward: 9.0, Epsilon: 0.70\n",
      "Episode: 71, Total Reward: 125.0, Epsilon: 0.70\n",
      "Episode: 72, Total Reward: 41.0, Epsilon: 0.70\n",
      "Episode: 73, Total Reward: 78.0, Epsilon: 0.69\n",
      "Episode: 74, Total Reward: 42.0, Epsilon: 0.69\n",
      "Episode: 75, Total Reward: 23.0, Epsilon: 0.69\n",
      "Episode: 76, Total Reward: 110.0, Epsilon: 0.68\n",
      "Episode: 77, Total Reward: 61.0, Epsilon: 0.68\n",
      "Episode: 78, Total Reward: 78.0, Epsilon: 0.68\n",
      "Episode: 79, Total Reward: 94.0, Epsilon: 0.67\n",
      "Episode: 80, Total Reward: 44.0, Epsilon: 0.67\n",
      "Episode: 81, Total Reward: 76.0, Epsilon: 0.67\n",
      "Episode: 82, Total Reward: 69.0, Epsilon: 0.66\n",
      "Episode: 83, Total Reward: 61.0, Epsilon: 0.66\n",
      "Episode: 84, Total Reward: 90.0, Epsilon: 0.66\n",
      "Episode: 85, Total Reward: 38.0, Epsilon: 0.65\n",
      "Episode: 86, Total Reward: 131.0, Epsilon: 0.65\n",
      "Episode: 87, Total Reward: 24.0, Epsilon: 0.65\n",
      "Episode: 88, Total Reward: 102.0, Epsilon: 0.64\n",
      "Episode: 89, Total Reward: 77.0, Epsilon: 0.64\n",
      "Episode: 90, Total Reward: 96.0, Epsilon: 0.64\n",
      "Episode: 91, Total Reward: 87.0, Epsilon: 0.63\n",
      "Episode: 92, Total Reward: 75.0, Epsilon: 0.63\n",
      "Episode: 93, Total Reward: 29.0, Epsilon: 0.63\n",
      "Episode: 94, Total Reward: 10.0, Epsilon: 0.62\n",
      "Episode: 95, Total Reward: 156.0, Epsilon: 0.62\n",
      "Episode: 96, Total Reward: 45.0, Epsilon: 0.62\n",
      "Episode: 97, Total Reward: 24.0, Epsilon: 0.61\n",
      "Episode: 98, Total Reward: 110.0, Epsilon: 0.61\n",
      "Episode: 99, Total Reward: 131.0, Epsilon: 0.61\n",
      "Episode: 100, Total Reward: 67.0, Epsilon: 0.61\n",
      "Episode: 101, Total Reward: 32.0, Epsilon: 0.60\n",
      "Episode: 102, Total Reward: 13.0, Epsilon: 0.60\n",
      "Episode: 103, Total Reward: 61.0, Epsilon: 0.60\n",
      "Episode: 104, Total Reward: 69.0, Epsilon: 0.59\n",
      "Episode: 105, Total Reward: 61.0, Epsilon: 0.59\n",
      "Episode: 106, Total Reward: 42.0, Epsilon: 0.59\n",
      "Episode: 107, Total Reward: 109.0, Epsilon: 0.58\n",
      "Episode: 108, Total Reward: 112.0, Epsilon: 0.58\n",
      "Episode: 109, Total Reward: 39.0, Epsilon: 0.58\n",
      "Episode: 110, Total Reward: 93.0, Epsilon: 0.58\n",
      "Episode: 111, Total Reward: 68.0, Epsilon: 0.57\n",
      "Episode: 112, Total Reward: 103.0, Epsilon: 0.57\n",
      "Episode: 113, Total Reward: 70.0, Epsilon: 0.57\n",
      "Episode: 114, Total Reward: 183.0, Epsilon: 0.56\n",
      "Episode: 115, Total Reward: 32.0, Epsilon: 0.56\n",
      "Episode: 116, Total Reward: 97.0, Epsilon: 0.56\n",
      "Episode: 117, Total Reward: 19.0, Epsilon: 0.56\n",
      "Episode: 118, Total Reward: 194.0, Epsilon: 0.55\n",
      "Episode: 119, Total Reward: 182.0, Epsilon: 0.55\n",
      "Episode: 120, Total Reward: 105.0, Epsilon: 0.55\n",
      "Episode: 121, Total Reward: 33.0, Epsilon: 0.55\n",
      "Episode: 122, Total Reward: 117.0, Epsilon: 0.54\n",
      "Episode: 123, Total Reward: 96.0, Epsilon: 0.54\n",
      "Episode: 124, Total Reward: 154.0, Epsilon: 0.54\n",
      "Episode: 125, Total Reward: 30.0, Epsilon: 0.53\n",
      "Episode: 126, Total Reward: 35.0, Epsilon: 0.53\n",
      "Episode: 127, Total Reward: 215.0, Epsilon: 0.53\n",
      "Episode: 128, Total Reward: 143.0, Epsilon: 0.53\n",
      "Episode: 129, Total Reward: 13.0, Epsilon: 0.52\n",
      "Episode: 130, Total Reward: 132.0, Epsilon: 0.52\n",
      "Episode: 131, Total Reward: 107.0, Epsilon: 0.52\n",
      "Episode: 132, Total Reward: 33.0, Epsilon: 0.52\n",
      "Episode: 133, Total Reward: 108.0, Epsilon: 0.51\n",
      "Episode: 134, Total Reward: 226.0, Epsilon: 0.51\n",
      "Episode: 135, Total Reward: 100.0, Epsilon: 0.51\n",
      "Episode: 136, Total Reward: 205.0, Epsilon: 0.51\n",
      "Episode: 137, Total Reward: 65.0, Epsilon: 0.50\n",
      "Episode: 138, Total Reward: 181.0, Epsilon: 0.50\n",
      "Episode: 139, Total Reward: 103.0, Epsilon: 0.50\n",
      "Episode: 140, Total Reward: 164.0, Epsilon: 0.50\n",
      "Episode: 141, Total Reward: 143.0, Epsilon: 0.49\n",
      "Episode: 142, Total Reward: 223.0, Epsilon: 0.49\n",
      "Episode: 143, Total Reward: 107.0, Epsilon: 0.49\n",
      "Episode: 144, Total Reward: 97.0, Epsilon: 0.49\n",
      "Episode: 145, Total Reward: 113.0, Epsilon: 0.48\n",
      "Episode: 146, Total Reward: 122.0, Epsilon: 0.48\n",
      "Episode: 147, Total Reward: 83.0, Epsilon: 0.48\n",
      "Episode: 148, Total Reward: 19.0, Epsilon: 0.48\n",
      "Episode: 149, Total Reward: 228.0, Epsilon: 0.47\n",
      "Episode: 150, Total Reward: 15.0, Epsilon: 0.47\n",
      "Episode: 151, Total Reward: 159.0, Epsilon: 0.47\n",
      "Episode: 152, Total Reward: 196.0, Epsilon: 0.47\n",
      "Episode: 153, Total Reward: 41.0, Epsilon: 0.46\n",
      "Episode: 154, Total Reward: 151.0, Epsilon: 0.46\n",
      "Episode: 155, Total Reward: 36.0, Epsilon: 0.46\n",
      "Episode: 156, Total Reward: 81.0, Epsilon: 0.46\n",
      "Episode: 157, Total Reward: 277.0, Epsilon: 0.46\n",
      "Episode: 158, Total Reward: 42.0, Epsilon: 0.45\n",
      "Episode: 159, Total Reward: 78.0, Epsilon: 0.45\n",
      "Episode: 160, Total Reward: 20.0, Epsilon: 0.45\n",
      "Episode: 161, Total Reward: 35.0, Epsilon: 0.45\n",
      "Episode: 162, Total Reward: 181.0, Epsilon: 0.44\n",
      "Episode: 163, Total Reward: 256.0, Epsilon: 0.44\n",
      "Episode: 164, Total Reward: 258.0, Epsilon: 0.44\n",
      "Episode: 165, Total Reward: 118.0, Epsilon: 0.44\n",
      "Episode: 166, Total Reward: 85.0, Epsilon: 0.44\n",
      "Episode: 167, Total Reward: 169.0, Epsilon: 0.43\n",
      "Episode: 168, Total Reward: 176.0, Epsilon: 0.43\n",
      "Episode: 169, Total Reward: 222.0, Epsilon: 0.43\n",
      "Episode: 170, Total Reward: 148.0, Epsilon: 0.43\n",
      "Episode: 171, Total Reward: 130.0, Epsilon: 0.42\n",
      "Episode: 172, Total Reward: 164.0, Epsilon: 0.42\n",
      "Episode: 173, Total Reward: 123.0, Epsilon: 0.42\n",
      "Episode: 174, Total Reward: 193.0, Epsilon: 0.42\n",
      "Episode: 175, Total Reward: 92.0, Epsilon: 0.42\n",
      "Episode: 176, Total Reward: 78.0, Epsilon: 0.41\n",
      "Episode: 177, Total Reward: 100.0, Epsilon: 0.41\n",
      "Episode: 178, Total Reward: 49.0, Epsilon: 0.41\n",
      "Episode: 179, Total Reward: 129.0, Epsilon: 0.41\n",
      "Episode: 180, Total Reward: 189.0, Epsilon: 0.41\n",
      "Episode: 181, Total Reward: 127.0, Epsilon: 0.40\n",
      "Episode: 182, Total Reward: 247.0, Epsilon: 0.40\n",
      "Episode: 183, Total Reward: 229.0, Epsilon: 0.40\n",
      "Episode: 184, Total Reward: 90.0, Epsilon: 0.40\n",
      "Episode: 185, Total Reward: 273.0, Epsilon: 0.40\n",
      "Episode: 186, Total Reward: 112.0, Epsilon: 0.39\n",
      "Episode: 187, Total Reward: 162.0, Epsilon: 0.39\n",
      "Episode: 188, Total Reward: 26.0, Epsilon: 0.39\n",
      "Episode: 189, Total Reward: 21.0, Epsilon: 0.39\n",
      "Episode: 190, Total Reward: 20.0, Epsilon: 0.39\n",
      "Episode: 191, Total Reward: 96.0, Epsilon: 0.38\n",
      "Episode: 192, Total Reward: 241.0, Epsilon: 0.38\n",
      "Episode: 193, Total Reward: 216.0, Epsilon: 0.38\n",
      "Episode: 194, Total Reward: 54.0, Epsilon: 0.38\n",
      "Episode: 195, Total Reward: 22.0, Epsilon: 0.38\n",
      "Episode: 196, Total Reward: 161.0, Epsilon: 0.37\n",
      "Episode: 197, Total Reward: 306.0, Epsilon: 0.37\n",
      "Episode: 198, Total Reward: 119.0, Epsilon: 0.37\n",
      "Episode: 199, Total Reward: 216.0, Epsilon: 0.37\n",
      "Episode: 200, Total Reward: 61.0, Epsilon: 0.37\n",
      "Episode: 201, Total Reward: 451.0, Epsilon: 0.37\n",
      "Episode: 202, Total Reward: 500.0, Epsilon: 0.36\n",
      "Episode: 203, Total Reward: 21.0, Epsilon: 0.36\n",
      "Episode: 204, Total Reward: 31.0, Epsilon: 0.36\n",
      "Episode: 205, Total Reward: 203.0, Epsilon: 0.36\n",
      "Episode: 206, Total Reward: 16.0, Epsilon: 0.36\n",
      "Episode: 207, Total Reward: 10.0, Epsilon: 0.35\n",
      "Episode: 208, Total Reward: 52.0, Epsilon: 0.35\n",
      "Episode: 209, Total Reward: 131.0, Epsilon: 0.35\n",
      "Episode: 210, Total Reward: 228.0, Epsilon: 0.35\n",
      "Episode: 211, Total Reward: 215.0, Epsilon: 0.35\n",
      "Episode: 212, Total Reward: 247.0, Epsilon: 0.35\n",
      "Episode: 213, Total Reward: 130.0, Epsilon: 0.34\n",
      "Episode: 214, Total Reward: 354.0, Epsilon: 0.34\n",
      "Episode: 215, Total Reward: 283.0, Epsilon: 0.34\n",
      "Episode: 216, Total Reward: 253.0, Epsilon: 0.34\n",
      "Episode: 217, Total Reward: 54.0, Epsilon: 0.34\n",
      "Episode: 218, Total Reward: 490.0, Epsilon: 0.34\n",
      "Episode: 219, Total Reward: 308.0, Epsilon: 0.33\n",
      "Episode: 220, Total Reward: 343.0, Epsilon: 0.33\n",
      "Episode: 221, Total Reward: 303.0, Epsilon: 0.33\n",
      "Episode: 222, Total Reward: 166.0, Epsilon: 0.33\n",
      "Episode: 223, Total Reward: 19.0, Epsilon: 0.33\n",
      "Episode: 224, Total Reward: 369.0, Epsilon: 0.33\n",
      "Episode: 225, Total Reward: 98.0, Epsilon: 0.32\n",
      "Episode: 226, Total Reward: 413.0, Epsilon: 0.32\n",
      "Episode: 227, Total Reward: 500.0, Epsilon: 0.32\n",
      "Episode: 228, Total Reward: 500.0, Epsilon: 0.32\n",
      "Episode: 229, Total Reward: 12.0, Epsilon: 0.32\n",
      "Episode: 230, Total Reward: 122.0, Epsilon: 0.32\n",
      "Episode: 231, Total Reward: 12.0, Epsilon: 0.31\n",
      "Episode: 232, Total Reward: 500.0, Epsilon: 0.31\n",
      "Episode: 233, Total Reward: 500.0, Epsilon: 0.31\n",
      "Episode: 234, Total Reward: 277.0, Epsilon: 0.31\n",
      "Episode: 235, Total Reward: 13.0, Epsilon: 0.31\n",
      "Episode: 236, Total Reward: 451.0, Epsilon: 0.31\n",
      "Episode: 237, Total Reward: 500.0, Epsilon: 0.30\n",
      "Episode: 238, Total Reward: 500.0, Epsilon: 0.30\n",
      "Episode: 239, Total Reward: 45.0, Epsilon: 0.30\n",
      "Episode: 240, Total Reward: 11.0, Epsilon: 0.30\n",
      "Episode: 241, Total Reward: 149.0, Epsilon: 0.30\n",
      "Episode: 242, Total Reward: 11.0, Epsilon: 0.30\n",
      "Episode: 243, Total Reward: 500.0, Epsilon: 0.30\n",
      "Episode: 244, Total Reward: 129.0, Epsilon: 0.29\n",
      "Episode: 245, Total Reward: 379.0, Epsilon: 0.29\n",
      "Episode: 246, Total Reward: 500.0, Epsilon: 0.29\n",
      "Episode: 247, Total Reward: 500.0, Epsilon: 0.29\n",
      "Episode: 248, Total Reward: 23.0, Epsilon: 0.29\n",
      "Episode: 249, Total Reward: 500.0, Epsilon: 0.29\n",
      "Episode: 250, Total Reward: 220.0, Epsilon: 0.29\n",
      "Training completed. Saved video.\n",
      "Moviepy - Building video ./video/dqn_cartpole.mp4.\n",
      "Moviepy - Writing video ./video/dqn_cartpole.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready ./video/dqn_cartpole.mp4\n"
     ]
    }
   ],
   "source": [
    "path_of_video_with_name = './video/dqn_cartpole.mp4'\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "video_recorder = VideoRecorder(env, path_of_video_with_name, enabled=True)\n",
    "\n",
    "num_episodes = 250\n",
    "max_steps = 500\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        env.render()\n",
    "        video_recorder.capture_frame()\n",
    "        \n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        agent.replay()\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    agent.decay_epsilon()\n",
    "    agent.update_target_model()\n",
    "\n",
    "    print(f\"ep: {episode + 1}, totalreward: {total_reward}, epsilion: {agent.epsilon:.2f}\")\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        torch.save(agent.model.state_dict(), f'./folder/dqn_model_episode_{episode}.pth')\n",
    "\n",
    "print(\"Training completed.\")\n",
    "\n",
    "video_recorder.close()\n",
    "video_recorder.enabled = False\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
